# Loading Nested API Data into Snowflake using `dlt` in Dagster

## Overview

This is a demo project that shows how to load nested data into Snowflake using `dlt` (Data Load Tool) in Dagster. It demonstrates the process of defining a `dlt` pipeline as a Dagster resource and implementing the pipeline with `dlt` resources and sources in Dagster assets. Additionally, it integrates AI analysis for sentiment assessment of textual data.

![Pipeline overview](https://storage.googleapis.com/dlt-blog-images/dlt_dagster_snowflake_demo_overview.png)

The diagram above represents the workflow overview of the project, encompassing the following steps:

1. Data loading from Google Trends and Hacker News to Snowflake using `dlt`, with an added step for OpenAI sentiment analysis specifically for Hacker News before the loading process.
2. Data reporting from the destination to a local directory in the form of image files. 

## Prerequisites

1. Snowflake credentials
    - username 
    - password
    - account and host

    >The host refers to your account identifier within your account. For instance, if your account is `https://kgiotue-wn98412.snowflakecomputing.com`, your host would be `kgiotue-wn98412`.

2. OpenAI API key
    
    >If you're new to [OpenAi](https://platform.openai.com/), they offer $5 in free credits usable during your first 3 months.
    
## Setup Guide

1. **Clone this repository**: Follow the instructions [here](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository).

2. **Create a virtual environment and activate it**: This step is advised to maintain a clean workspace and prevent dependency conflicts, although this is not mandatory.

   ```bash
    python -m venv myenv
    source myenv/bin activate
    ```
3. **Create a `secrets.toml` file in `.dlt` folder and enter the missing values**: Use the `example.secrets.toml` file for reference.

    > Deafault values for the role and warehouse are "ACCOUNTADMIN" and "COMPUTE_WH", respectively.

4. **Install dependencies**: Run the following command from the project folder.

   ```bash
    pip install -r requirements.txt
    ```

5. **Start Dagster server**: Run the following command.
   ```bash
    dagster dev
    ```
8. **Access Dagster UI**: Launch  http://127.0.0.1:3000.

    >If you want to run Dagster in cloud, or customize your project, consult their official [documentation](https://docs.dagster.io/getting-started).

## Understand Your Project

This project is very minimal, including just what's needed to run Dagster locally with `dlt`. Here's a quick breakdown of its structure:

1. `.dlt`: Utilized by the `dlt` library for storing configuration and sensitive information. The Dagster project is set up to fetch secret values from this directory as well.

2. `charts`: Used to store chart images generated by assets.

3. `dlt_dagster_snowflake_demo`: Your Dagster package, comprising Dagster assets, `dlt` resources, Dagster resources, and general project configurations.

### Dagster Resources Explained

In the `resources` folder, the following two Dagster resources are defined as classes:

1. `DltPipeline`: This is our `dlt` object defined as a Dagster ConfigurableResource that creates and runs a `dlt` pipeline with the specified data and table name. It will later be used in our Dagster assets to load data into Snowflake.

    ```python
    class DltPipeline(ConfigurableResource):
        # Initialize resource with pipeline details
        pipeline_name: str
        dataset_name: str
        destination: str

        def create_pipeline(self, resource_data, table_name):
            """
            Creates and runs a dlt pipeline with specified data and table name.
            
            Args:
                resource_data: The data to be processed by the pipeline.
                table_name: The name of the table where data will be loaded.
            
            Returns:
                The result of the pipeline execution.
            """

            # Configure the dlt pipeline with your destination details
            pipeline = dlt.pipeline(
                pipeline_name=self.pipeline_name,
                destination=self.destination,
                dataset_name=self.dataset_name
            )

            # Run the pipeline with your parameters
            load_info = pipeline.run(resource_data, table_name=table_name)
            return load_info
    ```

2. `LocalFileStorage`: Manages the local file storage, ensuring the storage directory exists and allowing data to be written to files within it. It will be later used in our Dagster assets to save images into the `charts` folder.

### `dlt` Explained

In the `dlt` folder within `dlt_dagster_snowflake_demo`, necessary `dlt` resources and sources are defined. Below is a visual representation illustrating the functionality of `dlt`:

![dlt explained](https://storage.googleapis.com/dlt-blog-images/dlt_dagster_snowflake_demo_dlt.png)

1. `hacker_news`: A `dlt` resource that yields stories related to specified orchestration tools from Hackernews. For each tool, it retrieves the top 5 stories that have at least one comment. The stories are then appended to the existing data.

    Note that the `write_disposition` can also be set to `merge` or `replace`: 
    - The merge write disposition merges the new data from the resource with the existing data at the destination. It requires a primary_key to be specified for the resource. More details can be found here.
    - The replace write disposition replaces the data in the destination with the data from the resource. It deletes all the classes and objects and recreates the schema before loading the data.

    More details can be found [here](https://dlthub.com/docs/general-usage/resource).

2. `comments`: A `dlt` transformer - a resource that receives data from another resource. It fetches comments for each story yielded by the `hacker_news` function. 

3. `hacker_news_full`: A `dlt` source that extracts data from the source location using one or more resource components, such as `hacker_news` and `comments`. To illustrate, if the source is a database, a resource corresponds to a table within that database.

4. `google_trends`: A `dlt` resource that fetches Google Trends data for specified orchestration tools. It attempts to retrieve the data multiple times in case of failures or empty responses. The retrieved data is then appended to the existing data.

As you may have noticed, the `dlt` library is designed to handle the unnesting of data internally. When you retrieve data from APIs like Hackernews or Google Trends, `dlt` automatically unpacks the nested structures into relational tables, creating and linking child and parent tables. This is achieved through unique identifiers (`_dlt_id` and `_dlt_parent_id`) that link child tables to specific rows in the parent table. However, it's important to note that you have control over [how this unnesting is done](https://dlthub.com/docs/general-usage/destination-tables).

### Dagster Assets Explained

> If you're new to Dagster, start by understanding the [concept of an asset](https://docs.dagster.io/concepts).

The assets defined in this project are essentially combinations of `dlt` resources paired with pipeline runs. When materialized, `dlt` objects are initialized, and the pipeline is executed to load data into Snowflake. This project includes the following assets:

1. `google_trends_asset`: Loads Google Trends data from the "google_trends" `dlt` resource to Snowflake using a `dlt` pipeline.

2. `google_trends_chart`: Generates a line chart visualizing Google Trends data over time and saves it to the local storage. This asset is dependent on `google_trends_asset`, since it uses data that's loaded by the latter. 

3. `hacker_news_full_asset`: Loads Hackernews data from the "hacker_news_full" `dlt` source to Snowflake using a `dlt` pipeline.

4. `hacker_news_chart`: Generates a line chart visualizing the sentiment of comments for each tool and saves it to the local storage.

4. `hacker_news_asset`: Separately loads Hackernews stories from the "hacker_news" `dlt` resource to Snowflake using a `dlt` pipeline.


## Materialize Your Assets

Once you launch your Dagster locally, you can materialize your assets and view the resulting charts in the corresponding folder. Feel free to explore and experiment with this project.

## View Your Data

Your data is now stored in Snowflake. It should look something like this:

![Snowflake view](https://storage.googleapis.com/dlt-blog-images/dlt_dagster_snowflake_demo_view.png)

## Contact / Support
For insights on executing custom pipelines using `dlt` or orchestrating workflows in Dagster, join their Slack communities:

- [dltHub](https://dlthub-community.slack.com)
- [Dagster](https://kestra-io.slack.com)

For more information on Snowflake, refer to their official [documentation](https://docs.snowflake.com/en/?_ga=2.9762677.1316386857.1709051821-830416446.1707924081).
